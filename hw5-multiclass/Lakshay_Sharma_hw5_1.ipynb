{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convex Surrogate Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Hinge loss is a convex surrogate for 0/1 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Let $f:\\mathcal X\\to\\mathbf R$ be a classification score function for binary\n",
    "classification. \n",
    "\n",
    "a) For any example $(x,y)\\in\\mathcal X\\times\\left\\{ -1,1\\right\\} $, show that\n",
    "$$\n",
    "1(y\\neq(f(x))\\le\\max\\left\\{ 0,1-yf(x)\\right\\} ,\n",
    "$$\n",
    "where $sign(x)=\\begin{cases}\n",
    "1 & x>0\\\\\n",
    "0 & x=0\\\\\n",
    "-1 & x<0\n",
    "\\end{cases}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    ">$$\n",
    "\\begin{array}{|r|r|r|r|} \\hline\n",
    " & sign(f(x)) & 1(y \\neq sign(f(x)) & max\\{0, 1-yf(x)\\} & 1(y\\neq(f(x))\\le\\max\\left\\{ 0,1-yf(x)\\right\\} \\\\ \\hline\n",
    " y=1 & & & & \\\\ \\hline\n",
    " & 1 & 0 & [0, 1) & True \\\\ \\hline\n",
    " & 0 & 1 & 1 & True \\\\ \\hline\n",
    " & -1 & 1 & (1, \\inf) & True \\\\ \\hline\n",
    " y \\neq 1 & & & & \\\\ \\hline\n",
    " & 1 & 1 & (1, \\inf) & True \\\\ \\hline\n",
    " & 0 & 1 & 1 & True \\\\ \\hline\n",
    " & -1 & 0 & [0, 1) & True \\\\ \\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)Show that the hinge loss $\\max\\left\\{ 0,1-m\\right\\} $ is a convex\n",
    "function of the margin $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> The hinge loss is the maximum of two functions: $0$, which is a constant function, and $1-m$, which is a linear function. Both of these are convex functions, and thus their point-wise maximum is also a convex function. Since its value is decided on the basis of the margin $m$, it is a convex function of the margin $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c)Suppose our prediction score functions are given by $f_{w}(x)=w^{T}x$.\n",
    "The hinge loss of $f_{w}$ on any example $\\left(x,y\\right)$ is then\n",
    "$\\max\\left\\{ 0,1-yw^{T}x\\right\\} $. Show that this is a convex function\n",
    "of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b> Solution </b>\n",
    "\n",
    "> Let $m(w)=yw^Tx$. Since $y$ and $x$ are fixed for any given sample point, we are correcting in saying that $m$ is a function of $w$. Since $yw^Tx$ is a linear function, and therefore convex, $m$ is a linear function of w. Therefore, similar to the previous problem, since hinge loss $max\\{0,1-yw^Tx\\}$ is thus the point-wise maximum of two convex functions, it is a convex function; since its value is decided on the basis of $w$, it is a convex function of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generalized Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Suppose we have chosen an $h\\in\\mathcal H$, from which we get the decision\n",
    "function $f(x)=argmax_{y\\in\\mathcal Y}h(x,y)$. Justify that for any $x\\in\\mathcal X$\n",
    "and $y\\in\\mathcal Y$, we have \n",
    "$$\n",
    "h(x,y)\\le h(x,f(x)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> For a given sample input $x$, $f(x)$ returns the specific $y \\in \\mathcal Y$ for which $h(x,y)$ has the highest possible value. Thus, $h(x,y)$ is like a score function, and $f(x)$ is the output/label class which has the highest output score for the given $x$. Therefore, by definition, for any given $x$, $h(x,f(x))\\geq h(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Justify the following two inequalities:\n",
    "\\begin{eqnarray*}\n",
    "\\Delta\\left(y,f(x)\\right) & \\le & \\Delta\\left(y,f(x)\\right)+h(x,f(x))-h(x,y)\\\\\n",
    " & \\le & \\max_{y'\\in\\mathcal Y}\\left[\\Delta\\left(y,y')\\right)+h(x,y')-h(x,y)\\right]\n",
    "\\end{eqnarray*}\n",
    "The RHS of the last expression is called the generalized hinge\n",
    "loss:\n",
    "$$\n",
    "\\ell\\left(h,\\left(x,y\\right)\\right)=\\max_{y'\\in\\mathcal Y}\\left[\\Delta\\left(y,y')\\right)+h(x,y')-h(x,y)\\right].\n",
    "$$\n",
    "We have shown that for any $x\\in\\mathcal X,y\\in \\mathcal Y,h\\in\\mathcal H$ we have\n",
    "$$\n",
    "\\ell\\left(h,(x,y)\\right)\\ge\\Delta(y,f(x)),\n",
    "$$\n",
    "where, as usual, $f(x)=argmax_{y\\in\\mathcal Y}h(x,y)$. [You should think\n",
    "about why we cannot write the generalized hinge loss as $\\ell\\left(f,(x,y)\\right)$.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> $\\Delta\\left(y,f(x)\\right) \\le \\Delta\\left(y,f(x)\\right)+h(x,f(x))-h(x,y)$\n",
    "\n",
    "> $\\Rightarrow 0 \\le h(x,f(x))-h(x,y)$\n",
    "\n",
    "> By definition, $f(x)$ is the argument returning the highest value when plugged in for $y$ in the expression $h(x, y)$.\n",
    "\n",
    "> Thus, the difference between $h(x,f(x))$ and $h(x,y)$ is always $\\geq 0$, and the equation above is justified.\n",
    "\n",
    "> Similarly, the second inequality  \n",
    "\n",
    "> $\\Delta\\left(y,f(x)\\right) \\le max_{y'\\in\\mathcal Y}\\left[\\Delta(y,y')+h(x,y')-h(x,y)\\right]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.We now introduce a specific base hypothesis space $\\mathcal H$ of linear\n",
    "functions. Consider a class-sensitive feature mapping $\\Psi:\\mathcal X\\times\\mathcal Y\\to\\mathbf R^{d}$,\n",
    "and $\\mathcal H=\\left\\{ h_{w}\\left(x,y\\right)=\\left\\langle w,\\Psi(x,y)\\right\\rangle \\mid w\\in\\mathbf R^{d}\\right\\} $.\n",
    "Show that we can write the generalized hinge loss for $h_{w}(x,y)$\n",
    "on example $\\left(x_{i},y_{i}\\right)$ as\n",
    "$$\n",
    "\\ell\\left(h_{w},(x_{i},y_{i})\\right)=\\max_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y)\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Solution </b>\n",
    "\n",
    "> The generalized hinge loss for this case is\n",
    "\n",
    "> $$\n",
    "\\ell\\left(h_{w},(x_{i},y_{i})\\right)=\\max_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y)\\right)+\\left\\langle w,\\Psi(x_{i},y) \\rangle - \\langle w, \\Psi(x_{i},y_{i})\\right\\rangle \\right].\n",
    "$$\n",
    "\n",
    "> where \n",
    "$\\left\\langle w,\\Psi(x_{i},y) \\rangle - \\langle w, \\Psi(x_{i},y_{i})\\right\\rangle$ can be rewritten as $\\left\\langle w,\\Psi(x_{i},y) -  \\Psi(x_{i},y_{i})\\right\\rangle$, thereby resulting in\n",
    "\n",
    "> $$\n",
    "\\ell\\left(h_{w},(x_{i},y_{i})\\right)=\\max_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y)\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.We will now show that the generalized hinge loss $\\ell\\left(h_{w},(x_{i},y_{i})\\right)$\n",
    "is a convex function of $w$. Justify each of the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) The expression $\\Delta(y_{i},y)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle $\n",
    "is an affine function of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> $\\Delta(y_i, y)$ is a scalar that remains constant with change in $w$, and similarly, $\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})$ is a vector that does not change with change in $w$. Since the expression $\\Delta(y_{i},y)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle $ is a linear combination of these constants varying with variation in $w$, it is an affine function of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)The expression $\\max_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y)\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right]$\n",
    "is a convex function of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b> Solution </b>\n",
    "\n",
    "> We proved above that $\\Delta(y_{i},y)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle $ is an affine function of $w$, which implies that it is a convex function. \n",
    "\n",
    "> Thus, for every possible $y \\in \\mathcal Y$, we have a resulting affine function of $w$. Since the point-wise maximum of a set of affine functions is also a convex function, $\\max_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y)\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right]$\n",
    "is a convex function of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Conclude that $\\ell\\left(h_{w},(x_{i},y_{i})\\right)$ is a convex\n",
    "surrogate for $\\Delta(y_{i},f_{w}(x_{i}))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> A convex surrogate loss function is a convex function that is an upper bound for the loss function of interest.\n",
    "\n",
    "> In 2.2.2, we proved that $\\ell (h_w, (x_i, y_i)) \\geq \\Delta(y_i, f_w(x_i))$, which means that $\\ell (h_w, (x_i, y_i))$ is an upper bound on the $\\Delta(y_i, f_w(x_i))$, which is our loss function of interest.\n",
    "\n",
    "> In 2.2.4.b, we proved that $\\ell (h_w, (x_i, y_i))$ is a convex function of $w$. \n",
    "\n",
    "> Thus, $\\ell (h_w, (x_i, y_i))$ is a convex surrogate $\\Delta (y_i,f_w(x_i))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SGD for Multiclass SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Suppose our output space and our action space are given as follows:\n",
    "$\\mathcal Y=\\mathcal A=\\left\\{ 1,\\ldots,k\\right\\} $. Given a non-negative class-sensitive\n",
    "loss function $\\Delta:\\mathcal Y\\times\\mathcal A\\to\\mathbf R^{\\ge0}$ and a class-sensitive\n",
    "feature mapping $\\Psi:\\mathcal X\\times\\mathcal Y\\to\\mathbf R^{d}$. Our prediction\n",
    "function $f:\\mathcal X\\to\\mathcal Y$ is given by\n",
    "$$\n",
    "f_{w}(x)=argmax_{y\\in\\mathcal Y}\\left\\langle w,\\Psi(x,y)\\right\\rangle \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.For a training set $(x_{1},y_{1}),\\ldots(x_{n},y_{n})$, let $J(w)$\n",
    "be the $\\ell_{2}$-regularized empirical risk function for the multiclass\n",
    "hinge loss. We can write this as\n",
    "$$\n",
    "J(w)=\\lambda\\|w\\|^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\max_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right].\n",
    "$$\n",
    "We will now show that $J(w)$ is a convex function of $w$. Justify\n",
    "each of the following steps. As we've shown it in a previous problem,\n",
    "you may use the fact that $w\\mapsto\\max_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right]$\n",
    "is a convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) $\\frac{1}{n}\\sum_{i=1}^{n}\\max_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right]$\n",
    "is a convex function of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Solution</b>\n",
    "\n",
    "> We have previous proved that $w\\mapsto\\max_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right]$. The expression in this problem is the sum of individual cconvex functions, and is thus convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) $\\|w\\|^{2}$ is a convex function of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Solution </b>\n",
    "\n",
    "> Norms are convex functions; therefore, the squares of norms are convex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) $J(w)$ is a convex function of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Solution </b>\n",
    "\n",
    "> A linear combination of convex functions is convex; therefore, $J(w)$ is a convext function of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Since $J(w)$ is convex, it has a subgradient at every point. Give\n",
    "an expression for a subgradient of $J(w)$. You may use any standard\n",
    "results about subgradients, including the result from an earlier homework\n",
    "about subgradients of the pointwise maxima of functions. (Hint: It\n",
    "may be helpful to refer to $\\hat{y}_{i}=argmax_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right]$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Solution </b>\n",
    "\n",
    "> $J(w)=\\lambda\\|w\\|^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\max_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y\\right)+\\left\\langle w,\\Psi(x_{i},y)-\\Psi(x_{i},y_{i})\\right\\rangle \\right].$\n",
    "\n",
    "> $\\Rightarrow J(w)=\\lambda w^T w+\\frac{1}{n}\\sum_{i=1}^{n}\\max_{y\\in\\mathcal Y}\\left[\\Delta\\left(y_{i},y\\right)+w^T \\Psi(x_{i},y)- w^T \\Psi(x_{i},y_{i}) \\right].$\n",
    "\n",
    "> Let us call the vector $g$ the subgradient of $J(w)$.\n",
    "\n",
    "> $\\Rightarrow g = 2 \\lambda w + \\frac{1}{n} \\sum_{i=1}^n \\Psi(x_i, y)-\\Psi(x_i, y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Give an expression the stochastic subgradient based on the point $(x_{i},y_{i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> $g_{SSGD} = 2 \\lambda w + \\Psi(x_i, y) - \\Psi(x_i, y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Give an expression for a minibatch subgradient, based on the points\n",
    "$(x_{i},y_{i}),\\ldots,\\left(x_{i+m-1},y_{i+m-1}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> $g_{Minibatch SGD} = 2\\lambda w + \\frac{1}{m} \\sum_{j=1}^{i+m-1} \\Psi(x_j, y) - \\Psi(x_j, y_j)$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
