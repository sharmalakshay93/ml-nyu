{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AdaBoost Actually Works [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Given training set $D=\\{(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\},$ where\n",
    "$y_{i}$'s are either $+1$ or $-1$, suppose we have a weak learner\n",
    "$G_{t}$ at time $t$ and we will perform $T$ rounds of AdaBoost.\n",
    "Initialize observation weights uniformly by setting $W^{1}=(w_{1}^{1},\\dots,w_{n}^{1})$\n",
    "with $w_{i}^{1}=1/n$ for $i=1,2,\\dots,n.$ For $t=1,2,\\dots,n$:\n",
    "\n",
    "1. Fit the weak learner $G_{t}$ at time $t$ to training set $D$ with\n",
    "weighting $W^{t}$.\n",
    "2. Compute the weighted misclassification error: $\\text{err}_{t}=\\sum_{D}w_{i}^{t}1{G_{t}(x_{i})\\neq y_{i}}/\\sum_{i}w_{i}^{t}$ \n",
    "3. Compute the contribution coefficient for the weak learner: $\\alpha_{t}=\\frac{1}{2}\\log(\\frac{1}{\\text{err}_{t}}-1)$ \n",
    "4. Update the weights: $w_{i}^{t+1}=w_{i}^{t}\\exp(-\\alpha_{t}y_{i}G_{t}(x_{i}))$ \n",
    "\n",
    "After $T$ steps, the cumulative contributions of weak learners is\n",
    "$G(x)=\\text{sign}(\\sum_{t=1}^{T}\\alpha_{t}G_{t}(x))$ as the final\n",
    "output. We will prove that with a reasonable weak learner the error\n",
    "of the output decreases exponentially fast with the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential bound on the training loss\n",
    "\n",
    "More precisely, we will show that the training error $L(G,D)=\\frac{1}{n}\\sum_{i=1}^{n}{1}_{\\{G(x_{i})\\neq y_{i}\\}}\\leq\\exp(-2\\gamma^{2}T)$\n",
    "where the error of the weak learner is less than $1/2-\\gamma$ for\n",
    "some $\\gamma>0$. To start, let's denote two cumulative variables:\n",
    "the output at time $t$ as $f_{t}=\\sum_{s\\leq t}\\alpha_{s}G_{s}$\n",
    "and $Z_{t}=\\frac{1}{n}\\sum_{i=1}^{n}\\exp(-y_{i}f_{t}(x_{i}))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.For any function $g$, show that $1_{\\{g(x)\\neq y\\}}<\\exp{(-yg(x))}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> Considering the equation in the problem\n",
    "\n",
    ">$$\n",
    "1_{\\{g(x)\\neq y\\}}<\\exp{(-yg(x))}\n",
    "$$\n",
    "\n",
    "> For $g(x)=y$, \n",
    "\n",
    "> $0 < e^{-1}$, so the condition holds.\n",
    "\n",
    "> For $g(x) \\neq y$,\n",
    "\n",
    "> $1 < e^1$, so the conidition holds.\n",
    "\n",
    "> Thus, for any function $g$, $1_{\\{g(x)\\neq y\\}}<\\exp{(-yg(x))}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Use this to show $L(G,D)<Z_{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> In the previous problem, we proved the inequality $1_{\\{g(x)\\neq y\\}}<\\exp{(-yg(x))}$ for any function $g$ and any $(x,y)$.\n",
    "\n",
    "> Summing over $n$ such $(x,y)$ pairs, the inequality would still hold.\n",
    "\n",
    "> $\\sum_{i=1}^n 1_{\\{g(x_i)\\neq y_i\\}}<\\sum_{i=1}^n\\exp{(-y_i g(x_i))}$\n",
    "\n",
    "> We can multiply with the constant $\\frac{1}{n}$ on the LHS and RHS, and the inequality would still hold.\n",
    "\n",
    "> $\\frac {1}{n}\\sum_{i=1}^n 1_{\\{g(x_i)\\neq y_i\\}}<\\frac{1}{n}\\sum_{i=1}^n\\exp{(-y_i g(x_i))}$\n",
    "\n",
    "> $L(G,D) < Z_T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Show that $w_{i}^{t+1}=\\exp(-y_{i}f_{t}(x_{i}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> From the problem, we have\n",
    "\n",
    "> $w_i^{t+1}=w_i^t exp(-y_i \\alpha_t G_t(x_i))$\n",
    "\n",
    "> $\\Rightarrow w_i^{t+1} = w_i^{t-1} exp(-y_i \\alpha_{t-1} G_{t-1}(x_i)) exp(-y_i \\alpha_t G_t(x_i))$\n",
    "\n",
    "> $\\Rightarrow w_i^{t+1} = w_i^{t-1} exp(-y_i \\alpha_{t-1} G_{t-1}(x_i)-y_i \\alpha_t G_t(x_i))$\n",
    "\n",
    "> $\\Rightarrow w_i^{t+1} = w_i^{t-1} exp\\left[-y_i (\\alpha_{t-1} G_{t-1}(x_i)+\\alpha_t G_t(x_i))\\right]$\n",
    "\n",
    "> $\\Rightarrow w_i^{t+1} = w_i^{t-1} exp\\left[-y_i \\sum_{s=t-1}^t \\alpha_s G_s(x_i) \\right]$\n",
    "\n",
    "> In this manner, we can keep expending the weight term for the previous iteration $t-1$, and will eventually be able to express the RHS as\n",
    "\n",
    "> $\\Rightarrow w_i^{t+1} = exp\\left[-y_i \\sum_{s \\leq t} \\alpha_s G_s(x_i) \\right]$\n",
    "\n",
    "> $\\Rightarrow w_i^{t+1} = exp\\left[-y_i f_t(x_i) \\right]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
