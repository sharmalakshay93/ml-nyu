{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the general gradient boosting algorithm, for a given loss function $\\ell$ and a hypothesis space $\\mathcal F$\n",
    "of regression functions (i.e. functions mapping from the input space\n",
    "to $\\mathbf R$): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Initialize $f_{0}(x)=0$.\n",
    "\n",
    "2) For $m=1$ to $M$:\n",
    "\n",
    "2.a) Compute: \n",
    "$$\n",
    "{\\bf g}_{m}=\\left(\\left.\\frac{\\partial}{\\partial f(x_{j})}\\sum_{i=1}^{n}\\ell\\left(y_{i},f(x_{i})\\right)\\right|_{f(x_{i})=f_{m-1}(x_{i}),\\,i=1,\\ldots,n}\\right)_{j=1}^{n}\n",
    "$$\n",
    "\n",
    "2.b) Fit regression model to $-{\\mathbf g}_{m}$: \n",
    "$$\n",
    "h_{m}=argmin_{h\\in\\mathcal F}\\sum_{i=1}^{n}\\left(\\left(-{\\bf g}_{m}\\right)_{i}-h(x_{i})\\right)^{2}.\n",
    "$$\n",
    "\n",
    "2.c) Choose fixed step size $\\nu_{m}=\\nu\\in(0,1]$, or take \n",
    "$$\n",
    "\\nu_{m}=argmin_{\\nu>0}\\sum_{i=1}^{n}\\ell\\left(y_{i},f_{m-1}(x_{i})+\\nu h_{m}(x_{i})\\right).\n",
    "$$\n",
    "\n",
    "2.d) Take the step: \n",
    "$$\n",
    "f_{m}(x)=f_{m-1}(x)+\\nu_{m}h_{m}(x)\n",
    "$$\n",
    "\n",
    "3) Return $f_{M}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we'll derive two special cases of the general gradient\n",
    "boosting framework: $L_{2}$-Boosting and BinomialBoost. \n",
    "\n",
    "1.Consider the regression framework, where $\\mathcal Y=\\mathbf R$. Suppose our\n",
    "loss function is given by \n",
    "$$\n",
    "\\ell(\\hat{y},y)=\\frac{1}{2}\\left(\\hat{y}-y\\right)^{2},\n",
    "$$\n",
    "and at the beginning of the $m$'th round of gradient boosting, we\n",
    "have the function $f_{m-1}(x)$. Show that the $h_{m}$ chosen as\n",
    "the next basis function is given by \n",
    "$$\n",
    "h_{m}=argmin_{h\\in\\mathcal F}\\sum_{i=1}^{n}\\left[\\left(y_{i}-f_{m-1}(x_{i})\\right)-h(x_{i})\\right]^{2}.\n",
    "$$\n",
    "In other words, at each stage we find the weak prediction function\n",
    "$h_{m}\\in\\mathcal F$ that is the best fit to the residuals from the previous\n",
    "stage. [Hint: Once you understand what's going on, this is a pretty\n",
    "easy problem.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> ${\\bf g}_{m} \\in R^n$ is the subgradient of $\\ell (\\hat{y},y)$ with respect to $f(x)$. \n",
    "\n",
    "> $$\n",
    "\\frac{\\partial \\ell(f(x),y)}{\\partial f(x)} = \\frac{\\partial {\\frac{1}{2}(f(x)-y)^2}}{\\partial f(x)}\n",
    "$$\n",
    "\n",
    "> $$\n",
    "= f(x)-y\n",
    "$$\n",
    "\n",
    "> Thus, ${\\bf g}_m \\in \\mathbf R^n$ is a vector whose $i^{th}$ component is given by (evaluated at $f_{m-1}$ at the $m^{th}$ step):\n",
    "\n",
    ">$$\n",
    "{\\bf g}_{m_i} = f_{m-1}(x_i)-y_i\n",
    "$$\n",
    "\n",
    "> Replacing the above in our equation for $h_m$:\n",
    "\n",
    ">$$\n",
    "h_{m}=argmin_{h\\in\\mathcal F}\\sum_{i=1}^{n}\\left(\\left(-{\\bf g}_{m}\\right)_{i}-h(x_{i})\\right)^{2}.\n",
    "$$\n",
    "\n",
    ">$$\n",
    "=argmin_{h\\in\\mathcal F}\\sum_{i=1}^{n}\\left(y_i-f_{m-1}(x_i)-h(x_{i})\\right)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Now let's consider the classification framework, where $\\mathcal Y=\\left\\{ -1,1\\right\\} $.\n",
    "In lecture, we noted that AdaBoost corresponds to forward stagewise\n",
    "additive modeling with the exponential loss, and that the exponential\n",
    "loss is not very robust to outliers (i.e. outliers can have a large\n",
    "effect on the final prediction function). Instead, let's consider\n",
    "the logistic loss \n",
    "$$\n",
    "\\ell(m)=\\ln\\left(1+e^{-m}\\right),\n",
    "$$\n",
    "where $m=yf(x)$ is the margin. Similar to what we did in the $L_{2}$-Boosting\n",
    "question, write an expression for $h_{m}$ as an argmin over $\\mathcal F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> When the loss is $\\ell(m)=\\ln\\left(1+e^{-yf(x)}\\right)$, its subgradient with respect to $f(x)$ is\n",
    "\n",
    "> $$\n",
    "\\frac{\\partial \\ell(f(x),y)}{\\partial f(x)} = \\frac{1}{1+e^{-f(x)y}}.{e^{-f(x)y}}.-y\n",
    "$$\n",
    "\n",
    "> $$\n",
    "\\frac{\\partial \\ell(f(x),y)}{\\partial f(x)} = -y . \\frac{e^{-f(x)y}}{1+e^{-f(x)y}}\n",
    "$$\n",
    "\n",
    ">$$\n",
    "\\frac{\\partial \\ell(f(x),y)}{\\partial f(x)} = \\frac{-y}{1+e^{f(x)y}}\n",
    "$$\n",
    "\n",
    "> Therefore, \n",
    "\n",
    ">$$\n",
    "{\\bf g}_{m_i} = \\frac{-y_i}{1+e^{f_{m-1}(x_i)y_i}}\n",
    "$$\n",
    "\n",
    ">Replacing the above in the equation for $h_m$:\n",
    "\n",
    ">$$\n",
    "h_{m}=argmin_{h\\in\\mathcal F}\\sum_{i=1}^{n}\\left(\\left(-{\\bf g}_{m}\\right)_{i}-h(x_{i})\\right)^{2}.\n",
    "$$\n",
    "\n",
    ">$$\n",
    "=argmin_{h\\in\\mathcal F}\\sum_{i=1}^{n}\\left(\\frac{y_i}{1+e^{f_{m-1}(x_i)y_i}}-h(x_{i})\\right)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. From Margins to Conditional Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Below we'll calculate $f^{*}$ for several loss functions. It will\n",
    "be convenient to let $\\pi(x)=\\mathbb P \\left(y=1\\mid x\\right)$ in the work\n",
    "below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Write $\\mathbb E_{y}\\left[\\ell\\left(yf(x)\\right)\\mid x\\right]$ in terms\n",
    "of $\\pi(x)$ and $\\ell\\left(f(x)\\right)$. [Hint: Use the fact that\n",
    "$y\\in\\left\\{ -1,1\\right\\} $.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> Let $\\pi(x) = \\mathbb P(y=1|x) = p$, and $f(x)=\\hat{y}$.\n",
    "\n",
    "> Then, \n",
    "$$\\mathbb E_{y}\\left[\\ell\\left(y\\hat{y}\\right)\\mid x\\right] = \\sum_{y} P(y|x) . l(y\\hat{y})$$\n",
    "\n",
    ">$$ = P(y=1|x) l(1.\\hat{y}) + P(y=-1|x) l(-1.\\hat{y})$$\n",
    "\n",
    ">$$\n",
    "= p.l(\\hat{y})+(1-p).l(-\\hat{y})\n",
    "$$\n",
    "\n",
    "\n",
    ">$$\n",
    "= \\pi(x).l(\\hat{y})+(1-\\pi(x)).l(-\\hat{y})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Show that the Bayes prediction function $f^{*}(x)$ for the exponential\n",
    "loss function $\\ell\\left(y,f(x)\\right)=e^{-yf(x)}$ is given by \n",
    "$$\n",
    "f^{*}(x)=\\frac{1}{2}\\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n",
    "$$\n",
    "and, given the Bayes prediction function $f^{*}$, we can recover\n",
    "the conditional probabilities by\n",
    "$$\n",
    "\\pi(x)=\\frac{1}{1+e^{-2f^{*}(x)}}.\n",
    "$$\n",
    "[Hint: Differentiate the expression in the previous problem with\n",
    "respect to $f(x)$. To make things a little less confusing, and also\n",
    "to write less, you may find it useful to change variables a bit: Fix\n",
    "an $x\\in\\mathcal X$. Then write $p=\\pi(x)$ and $\\hat{y}=f(x)$. After substituting\n",
    "these into the expression you had for the previous problem, you'll\n",
    "want to find $\\hat{y}$ that minimizes the expression. Use differential\n",
    "calculus. Once you've done it for a single $x$, it's easy to write\n",
    "the solution as a function of $x$.] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><b>Solution</b>\n",
    "\n",
    "> As specified above, taking some fixed $x \\in \\mathcal X$, and writing $p=\\pi(x)$ and $\\hat{y}=f(x)$, we replace the loss function $l(y\\hat{y})$ with $e^{-y\\hat{y}}$in expression we derived in the previous problem.\n",
    "\n",
    "> $\n",
    "\\mathbb E_{y}\\left[\\ell\\left(y\\hat{y}\\right)\\mid x\\right] = \\pi(x).l(\\hat{y})+(1-\\pi(x)).l(-\\hat{y})\n",
    "$\n",
    "\n",
    ">$\n",
    "= p.e^{-\\hat{y}}+(1-p).e^{\\hat{y}}\n",
    "$\n",
    "\n",
    "> Differentiating w.r.t. $\\hat{y}$, equating with $0$, and solving for $\\hat{y}$ gives us the Bayes prediction function $f^*(x)$:\n",
    "\n",
    "> $\n",
    "\\Rightarrow -pe^{-\\hat{y}}+(1-p)e^{\\hat{y}} = 0\n",
    "$\n",
    "\n",
    "> $\n",
    "pe^{-\\hat{y}}=(1-p)e^{\\hat{y}}\n",
    "$\n",
    "\n",
    "> $\n",
    "ln(pe^{-\\hat{y}}) = ln((1-p)e^{\\hat{y}})\n",
    "$\n",
    "\n",
    "> $\n",
    "ln(p) + ln(e^{-\\hat{y}}) = ln(1-p)+ln(e^\\hat{y})\n",
    "$\n",
    "\n",
    "\n",
    "> $\n",
    "ln(p) -\\hat{y} = ln(1-p)+\\hat{y}\n",
    "$\n",
    "\n",
    "\n",
    "> $\n",
    "2\\hat{y} = ln(p)-ln(1-p)\n",
    "$\n",
    "\n",
    "\n",
    "> $\n",
    "2\\hat{y} = ln\\left(\\frac{p}{1-p}\\right)\n",
    "$\n",
    "\n",
    "> $\n",
    "\\hat{y} = \\frac{1}{2}ln\\left(\\frac{p}{1-p}\\right)\n",
    "$\n",
    "\n",
    "> $\n",
    "f^*(x) = ln\\left(\\frac{p}{1-p}\\right)\n",
    "$\n",
    "\n",
    "> $\n",
    "f^*(x) = ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n",
    "$\n",
    "\n",
    ">Solving for $\\pi(x)$\n",
    "\n",
    ">$2f^*(x)= ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)$\n",
    "\n",
    "> $e^{2f^*(x)}= \\frac{\\pi(x)}{1-\\pi(x)}$\n",
    "\n",
    "> $\\pi(x)=e^{2f^*(x)}-\\pi(x)e^{2f^*(x)}$\n",
    "\n",
    "> $\\pi(x) + \\pi(x)e^{2f^*(x)} = e^{2f^*(x)}$\n",
    "\n",
    "> $\\pi(x) (1 + e^{2f^*(x)}) = e^{2f^*(x)}$\n",
    "\n",
    "> $\\pi(x) = \\frac{e^{2f^*(x)}}{1 + e^{2f^*(x)}}$\n",
    "\n",
    "> $\\pi(x) = \\frac{1}{1 + e^{-2f^*(x)}}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Show that the Bayes prediction function $f^{*}(x)$ for the logistic\n",
    "loss function $\\ell\\left(y,f(x)\\right)=\\ln\\left(1+e^{-yf(x)}\\right)$\n",
    "is given by\n",
    "$$\n",
    "f^{*}(x)=\\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n",
    "$$\n",
    "and the conditional probabilities are given by\n",
    "$$\n",
    "\\pi(x)=\\frac{1}{1+e^{-f^{*}(x)}}.\n",
    "$$\n",
    "Again, we may assume that $\\pi(x)\\in(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> $\n",
    "\\mathbb E_{y}\\left[\\ell\\left(y\\hat{y}\\right)\\mid x\\right] = p.l(\\hat{y})+(1-).l(-\\hat{y})\n",
    "$\n",
    "\n",
    "> $\n",
    "= p.ln(1+e^{-\\hat{y}})+(1-p).ln(1+e^{\\hat{y}})\n",
    "$\n",
    "\n",
    "> Differentiating w.r.t. $\\hat{y}$, equating with $0$, and solving for $\\hat{y}$ gives us the Bayes prediction function $f^*(x)$:\n",
    "\n",
    "> $\n",
    "\\Rightarrow \\partial_{\\hat{y}}(p.ln(1+e^{-\\hat{y}})+(1-p).ln(1+e^{\\hat{y}})) = 0\n",
    "$\n",
    "\n",
    "> $\n",
    "\\Rightarrow \\frac{1}{1+e^{-\\hat{y}}}.-pe^{-\\hat{y}} + \\frac{1}{1+e^{\\hat{y}}}.(1-p)e^{\\hat{y}} = 0\n",
    "$\n",
    "\n",
    ">$\n",
    "\\Rightarrow \\frac{(1-p)e^{\\hat{y}}}{1+e^{\\hat{y}}} = \\frac{pe^{-\\hat{y}}}{1+e^{-\\hat{y}}}\n",
    "$\n",
    "\n",
    ">$\n",
    "\\Rightarrow \\frac{(1-p)e^{\\hat{y}}}{1+e^{\\hat{y}}} = \\frac{p}{e^{\\hat{y}}+1}\n",
    "$\n",
    "\n",
    ">$\n",
    "\\Rightarrow (1-p)e^{\\hat{y}} = p\n",
    "$\n",
    "\n",
    ">$\n",
    "\\Rightarrow e^{\\hat{y}} = \\frac{p}{1-p}\n",
    "$\n",
    "\n",
    ">$\n",
    "\\Rightarrow \\hat{y} = ln\\left(\\frac{p}{1-p}\\right)\n",
    "$\n",
    "\n",
    ">$\n",
    "\\Rightarrow f^*(x) = ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n",
    "$\n",
    "\n",
    "> Solving for $\\pi(x)$\n",
    "\n",
    "> $\\Rightarrow e^{f^*(x)} = \\frac{\\pi(x)}{1-\\pi(x)}$\n",
    "\n",
    "> $\\Rightarrow e^{f^*(x)} - \\pi(x) e^{f^*(x)}  = \\pi(x)$\n",
    "\n",
    "> $\\Rightarrow e^{f^*(x)} = \\pi(x) + \\pi(x) e^{f^*(x)}$\n",
    "\n",
    "> $\\Rightarrow \\pi(x) = \\frac{e^{f^*(x)}}{1+e^{f^*(x)}}$\n",
    "\n",
    "> $\\Rightarrow \\pi(x) = \\frac{1}{1+e^{-f^*(x)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.[Optional] Show that the Bayes prediction function $f^{*}(x)$\n",
    "for the hinge loss function $\\ell\\left(y,f(x)\\right)=\\max\\left(0,1-yf(x)\\right)$\n",
    "is given by\n",
    "$$\n",
    "f^{*}(x)=sign\\left(\\pi(x)-\\frac{1}{2}\\right).\n",
    "$$\n",
    "Note that it is impossible to recover $\\pi(x)$ from $f^{*}(x)$ in\n",
    "this scenario. However, in practice we work with an empirical risk\n",
    "minimizer, from which we may still be able to recover a reasonable\n",
    "estimate for $\\pi(x)$. An early approach to this problem is known\n",
    "as \"Platt scaling\"(https://en.wikipedia.org/wiki/Platt_scaling}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> As before,\n",
    "\n",
    "> $\n",
    "\\mathbb E_{y}\\left[\\ell\\left(y\\hat{y}\\right)\\mid x\\right] = p.l(\\hat{y})+(1-p).l(-\\hat{y})\n",
    "$\n",
    "\n",
    "> $\n",
    "= p.max(0, 1-\\hat{y})+(1-p).max(0,1+\\hat{y})\n",
    "$\n",
    "\n",
    "> Using Wolfram Alpha, I found the minimizer for thise function to $\\hat{y}=1$ and $\\hat{y}=-1$. At these points, the function evalutes to\n",
    "\n",
    "> $p.max(0, 1-1) + (1-p).max(0,1+1) = 2(1-p)$ for $\\hat{y} =1$\n",
    "\n",
    "> and \n",
    "\n",
    "> $p.max(0, 1+1) + (1-p).max(0,1-1) = 2p$ for $\\hat{y} =-1$\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
