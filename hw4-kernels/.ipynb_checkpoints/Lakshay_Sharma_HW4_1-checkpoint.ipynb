{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Positive Semidefinite Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Give an example of an orthogonal matrix that is not symmetric. (Hint:\n",
    "You can use a $2\\times2$ matrix with only the entries -1, 0, and\n",
    "1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> $M=\\begin{pmatrix}0 & 1\\\\\n",
    "-1 & 0\\\\\n",
    "\\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Use the definition of a psd matrix and the spectral theorem to show\n",
    "that all eigenvalues of a positive semidefinite matrix $M$ are non-negative.\n",
    "[Hint: By Spectral theorem, $\\Sigma=Q^{T}MQ$ for some $Q$. What\n",
    "if you take $A=Q$ in the \"exercise in matrix multiplication\"\n",
    "described above?] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> Say $M \\in \\mathbf{R}^{nxn}$ is a psd matrix. By the spectral theorem, since M is real and symmetric, it can be expressed as $M=Q\\Sigma Q^{T}$, where $\\Sigma$ is a diagonal matrix of the eigenvalues of $M$. \n",
    "\n",
    "> Pre-multiplying and post-multiplying with $Q^T$ and $Q$ respectively, the equation becomes $Q^TMQ=\\Sigma$. Since M is psd, we $diag(Q^T M Q) = diag(\\Sigma) \\succeq 0$. \n",
    "\n",
    "> Thus, all eigenvalues of a psd matrix $M$ are non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3.In this problem we show that a psd matrix is a matrix version of a\n",
    "non-negative scalar, in that they both have a \"square root\". Show\n",
    "that a symmetric matrix $M$ can be expressed as $M=BB^{T}$ for some\n",
    "matrix $B$, if and only if $M$ is psd. [Hint: To show $M=BB^{T}$\n",
    "implies $M$ is psd, use the fact that for any vector $v$, $v^{T}v\\ge0$.\n",
    "To show that $M$ psd implies $M=BB^{T}$ for some $B$, use the Spectral\n",
    "Theorem.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> Let us first try to prove that a symmetric matrix that can be expressed as $BB^T$ is positive semidefinite.\n",
    "\n",
    "> Given that $M$ is a symmetric matirx, and can be expressed as\n",
    "\n",
    "> $M = BB^T$\n",
    "\n",
    "> We can rewrite this equation as\n",
    "\n",
    "> $x^T Mx = x^TBB^Tx$\n",
    "\n",
    "> $x^T Mx = (B^T x)^T B^Tx$\n",
    "\n",
    "> $x^T Mx = ||B^T x||_2 ^2 \\geq0$\n",
    "\n",
    "> Thus, since $x^T Mx \\geq 0$, $M$ is a psd. This proved that a symmetric matrix $M$ that can be expressed using a matrix $B$ as $BB^T$ is always positive semidefinite.\n",
    "\n",
    "> Now, let us consider that $M$ is positive semidefinite, and try to prove that it can be expressed as $BB^T$.\n",
    "\n",
    "> If $M$ is known to be psd, then, by Spectral Theorem, it can be expressed as: \n",
    "\n",
    "> $M = Q \\Sigma Q^T$, where $Q$ is an orthogonal matrix, and $\\Sigma$ is a diagonal matrix of the eigenvalues of M.\n",
    "\n",
    "> We have proved above that all the eigenvalues of a psd matrix are non-negative. Since $\\Sigma$ is a diagonal matrix consisting of all all non-negative elements along the diagon, we can express it as the product of two identical diagonal matrices, where the values along the diagonal are the square roots of those in $\\Sigma$. Thus, $M$ can be expressed as:\n",
    "\n",
    "> $M = Q \\Sigma^{1/2} \\Sigma Q^T$\n",
    "\n",
    "> We can say $Q \\Sigma^{1/2} = B$, and rewrite the above equation as:\n",
    "\n",
    "> $M = BB^T$\n",
    "\n",
    "> Thus, we have proved that if M is psd, it can always be expressed in the form $BB^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Positive Definite Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "A real, symmetric matrix $M\\in\\mathbf{R}^{n\\times n}$ is $\\textbf{positive\n",
    "definite}$ (spd) if for any $x\\in \\mathbf {R}^{n}$ with $x\\neq0$, \n",
    "\n",
    "$x^{T}Mx>0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Show that all eigenvalues of a symmetric positive definite matrix\n",
    "are positive. [Hint: You can use the same method as you used for\n",
    "psd matrices above.] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> Say $M \\in \\mathbf{R}^{nxn}$ is a positive definite matrix. By the spectral theorem, since M is real and symmetric, it can be expressed as $M=Q\\Sigma Q^{T}$, where $\\Sigma$ is a diagonal matrix of the eigenvalues of $M$. \n",
    "\n",
    "> Pre-multiplying and post-multiplying with $Q^T$ and $Q$ respectively, the equation becomes $Q^TMQ=\\Sigma$. Since M is positive definite, we $diag(Q^T M Q) = diag(\\Sigma) \\succ 0$. \n",
    "\n",
    "> Thus, all eigenvalues of a positive definite matrix $M$ are non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Let $M$ be a symmetric positve definite matrix. By the spectral theorem,\n",
    "$M=Q\\Sigma Q^{T}$, where $\\Sigma$ is a diagonal matrix of the eigenvalues\n",
    "of $M$. By the previous problem, all diagonal entries of $\\Sigma$\n",
    "are positive. If $\\Sigma=diag\\left(\\sigma_{1},\\ldots,\\sigma_{n}\\right)$,\n",
    "then $\\Sigma^{-1}=diag\\left(\\sigma_{1}^{-1},\\ldots,\\sigma_{n}^{-1}\\right)$.\n",
    "Show that the matrix $Q\\Sigma^{-1}Q^{T}$ is the inverse of $M$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> We know that $Q$ is an orthogonal matrix whose columns are a set of orthonormal eignevectors of M. Since $Q$ is an orthogonal matrix, this implies $Q^T = Q^{-1}$. \n",
    "\n",
    "> We can say that $M^-1 = Q\\Sigma^{-1} Q^T$ if we can show that $(Q\\Sigma {-1}Q^T)M = \\mathbb{I}_n$\n",
    "\n",
    "> $(Q\\Sigma {-1}Q^T)M$\n",
    "\n",
    "> $\\Rightarrow Q\\Sigma^ {-1}Q^T Q\\Sigma Q^T$\n",
    "\n",
    "> $\\Rightarrow Q\\Sigma^ {-1}Q^{-1} Q\\Sigma Q^T$            (since $Q^T = Q^{-1}$)\n",
    "\n",
    "> $\\Rightarrow Q\\Sigma^ {-1}\\Sigma Q^T$\n",
    "\n",
    "> $\\Rightarrow Q Q^T$\n",
    "\n",
    "> $\\Rightarrow Q Q^{-1}$ (since $Q^T = Q^{-1}$)\n",
    "\n",
    "> $\\Rightarrow \\mathbb I_n$ (since matrix inverse multiplication is commutative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Since positive semidefinite matrices may have eigenvalues that are\n",
    "zero, we see by the previous problem that not all psd matrices are\n",
    "invertible. Show that if $M$ is a psd matrix and $I$ is the identity\n",
    "matrix, then $M+\\lambda I$ is symmetric positive definite for any\n",
    "$\\lambda>0$, and give an expression for the inverse of $M+\\lambda I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> Since $M$ is psd, $M + \\lambda \\mathbb I$ is psd, since the sum of two psd matrices is also psd.\n",
    "\n",
    "> Since $M + \\lambda \\mathbb I$ is psd, it is real and symmetric, and thus by the Spectral Theorem, can be expressed as $M+ \\lambda \\mathbb I=Q\\Sigma Q^T$.\n",
    "\n",
    "> We have also shown that all eigenvalues of $M$ are $\\geq0$\n",
    "\n",
    "> $M + \\lambda \\mathbb I$\n",
    "\n",
    "> $\\Rightarrow M + \\lambda QQ^T$ (since $Q$ is an orthogonal matrix, and thus $Q^T = Q^{-1}$\n",
    "\n",
    "> $\\Rightarrow Q \\Sigma Q^T + \\lambda QQ^T$\n",
    "\n",
    "> $\\Rightarrow Q \\Sigma Q^T + Q \\lambda \\mathbb {I} Q^T$\n",
    "\n",
    "> $\\Rightarrow Q (\\Sigma + \\lambda \\mathbb {I}) Q^T$\n",
    "\n",
    "> Since $M + \\lambda \\mathbb{I}$ is psd, $\\Sigma + \\lambda \\mathbb {I}$ is a diagonal matrix whose diagonal elements are the eigenvalues of $M + \\lambda \\mathbb{I}$. Since all of the eigenvalues are non-negative for a psd matrix, all the diagonal elements of $\\Sigma + \\lambda \\mathbb {I}$ are non-negative.\n",
    "\n",
    "> However, since we have been told that $\\lambda > 0$,  $diag(\\Sigma + \\lambda \\mathbb {I} > 0)$. \n",
    "\n",
    "> We have proved above that if a psd matrix has strictly positive eigenvalues, it is a positive definite matrix. Thus, $M+\\lambda \\mathbb I$ is a positive definite matrix.\n",
    "\n",
    "> Also, since $\\Sigma + \\lambda \\mathbb {I}$ is a diagonal matrix with all diagonal elements being positive, $(\\Sigma+\\lambda \\mathbb I)^{-1}$ always exists, and equals $Q(\\Sigma+\\lambda \\mathbb I)Q^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Let $M$ and $N$ be symmetric matrices, with $M$ positive semidefinite\n",
    "and $N$ positive definite. Use the definitions of psd and spd to\n",
    "show that $M+N$ is symmetric positive definite. Thus $M+N$ is invertible.\n",
    "(Hint: For any $x\\neq0$, show that $x^{T}(M+N)x>0$. Also note that\n",
    "$x^{T}(M+N)x=x^{T}Mx+x^{T}Nx$.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since $M \\in \\mathbf{R}^n$ is positive semidefinite, $x^T M x \\geq 0$ for any $x \\in \\mathbf R$.\n",
    "\n",
    "> Since $M \\in \\mathbf{R}^n$ is positive definite, $x^T M x > 0$ for any $x \\in \\mathbf R$.\n",
    "\n",
    "> Thus, $x^{T}Mx+x^{T}Nx = x^{T}(M+N)x > 0$ \n",
    "\n",
    "> This proves $M+N$ is positive definite, and thus invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.[Optional] Kernel Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following problem will gives us some additional insight into\n",
    "what information is encoded in the kernel matrix. \n",
    "\n",
    "1.[Optional] Consider a set of vectors $S=\\{x_{1},\\ldots,x_{m}\\}$.\n",
    "Let $X$ denote the matrix whose rows are these vectors. Form the\n",
    "Gram matrix $K=XX^{T}$. Show that knowing $K$ is equivalent to knowing\n",
    "the set of pairwise distances among the vectors in $S$ as well as\n",
    "the vector lengths. [Hint: The distance between $x$ and $y$ is\n",
    "given by $d(x,y)=\\|x-y\\|$, and the norm of a vector $x$ is defined\n",
    "as $\\|x\\|=$$\\sqrt{\\left\\langle x,x\\right\\rangle }=\\sqrt{x^{T}x}$.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    ">$X=\\begin{pmatrix} - x_1 -\\\\\n",
    "- x_2 -\\\\\n",
    "\\vdots \\\\\n",
    "- x_m -\n",
    "\\end{pmatrix}$\n",
    "\n",
    "> $K = XX^T$\n",
    "\n",
    "> $=\\begin{pmatrix} - x_1 -\\\\\n",
    "- x_2 -\\\\\n",
    "\\vdots \\\\\n",
    "- x_m -\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix} \n",
    "|& | & & | \\\\\n",
    "x_1 & x_2 & \\cdots & x_m \\\\\n",
    "|& | & & |\n",
    "\\end{pmatrix}$\n",
    "\n",
    "> $=\\begin{pmatrix}\n",
    "x_1^T x_1 & x_1^T x_2 & \\cdots & x_1^T x_m\\\\\n",
    "x_2^T x_1 & x_2^T x_2 & \\cdots & x_2^T x_m\\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "x_m^T x_1 & x_m^T x_2 & \\cdots & x_m^T x_m\\\\\n",
    "\\end{pmatrix} $\n",
    "\n",
    "> $=\\begin{pmatrix}\n",
    "\\langle x_1, x_1 \\rangle & \\langle x_1, x_2 \\rangle  & \\cdots & \\langle x_1, x_m \\rangle \\\\\n",
    "\\langle x_2, x_1 \\rangle & \\langle x_2, x_2 \\rangle  & \\cdots & \\langle x_2^T, x_m \\rangle \\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "\\langle x_m, x_1 \\rangle & \\langle x_m, x_2 \\rangle  & \\cdots & \\langle x_m, x_m \\rangle \\\\\n",
    "\\end{pmatrix} $\n",
    "\n",
    "> Since the distance between $x$ and $y$ is\n",
    "given by $d(x,y)=\\|x-y\\|$, and the norm of a vector $x$ is defined\n",
    "as $\\|x\\|=$$\\sqrt{\\left\\langle x,x\\right\\rangle }=\\sqrt{x^{T}x}$,\n",
    "\n",
    "> $d(x_i,x_j)=\\|x_i-x_j\\|_2$\n",
    "\n",
    "> $=\\sqrt {\\langle {x_i - x_j, x_i - x_j}\\rangle}$\n",
    "\n",
    "> $=\\sqrt {\\langle {x_i, x_i}\\rangle - 2\\langle {x_i, x_j}\\rangle + \\langle{x_j, x_j}\\rangle}$\n",
    "\n",
    "> All of the above terms can be found in $K$. Thus, for any given pair of vectors $x_i$ and $x_j$ in $S$, we have the distance between them encoded within $K$.\n",
    " \n",
    "> Also, the length of any given vector $x_i$ in $S$ is $\\langle x_i, x_i \\rangle$. All elements of such form in $K$ can be found among the diagonal elements of $K$. \n",
    "\n",
    "> Therefore, knowing $K$ is equivalent to knowing\n",
    "the set of pairwise distances among the vectors in $S$ as well as\n",
    "the vector lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, we discussed how to kernelize ridge regression using the\n",
    "representer theorem. Here we pursue a bare-hands approach. \n",
    "\n",
    "Suppose our input space is $\\mathcal X = \\mathbf R^{d}$ and\n",
    "our output space is $\\mathcal Y=\\mathbf R$. Let $\\mathcal D=\\left\\{ \\left(x_{1},y_{1}\\right),\\ldots,\\left(x_{n},y_{n}\\right)\\right\\} $\n",
    "be a training set from $\\mathcal X \\times \\mathcal Y$. We'll use the ''design matrix''\n",
    "$X\\in\\mathbf R^{n\\times d}$, which has the input vectors as rows: \n",
    "\n",
    "\n",
    "$X=\\begin{pmatrix}-x_{1}-\\\\\n",
    "\\vdots\\\\\n",
    "-x_{n}-\n",
    "\\end{pmatrix}.$\n",
    "\n",
    "\n",
    "Recall the ridge regression objective function:\n",
    "\n",
    "$J(w)=||Xw-y||^{2}+\\lambda||w||^{2},\n",
    "$ for $\\lambda>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Show that for $w$ to be a minimizer of $J(w)$, we must have $X^{T}Xw+\\lambda Iw=X^{T}y$.\n",
    "Show that the minimizer of $J(w)$ is $w=(X^{T}X+\\lambda I)^{-1}X^{T}y$.\n",
    "Justify that the matrix $X^{T}X+\\lambda I$ is invertible, for $\\lambda>0$.\n",
    "(The last part should follow easily from the earlier exercises on\n",
    "psd and spd matrices.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> $J(w)=||Xw-y||^{2}+\\lambda||w||^{2}$\n",
    "\n",
    "> $\\Rightarrow J(w)= (Xw-y)^T (Xw-y)+\\lambda w^{T}w$\n",
    "\n",
    "> In order for $w$ to be a minimizer, the gradient of $J(w)$ must equal zero.\n",
    "\n",
    "> $\\Rightarrow \\frac{\\nabla J(w)}{\\partial w} = 0$\n",
    "\n",
    "> $\\Rightarrow 2X^T(Xw-y) + 2 \\lambda \\mathbb I w = 0$\n",
    "\n",
    "> $\\Rightarrow 2X^T Xw - 2X^T y + 2 \\lambda w = 0$\n",
    "\n",
    "> $\\Rightarrow X^T Xw + \\lambda \\mathbb I w = X^T y$\n",
    "\n",
    "> $\\Rightarrow w = (X^T X + \\lambda \\mathbb I)^{-1} X^T y$\n",
    "\n",
    "> $X^T X$ and $\\lambda \\mathbb I$ are both square, real matrices, which are also positive semidefinite. Thus, their sum is also psd. \n",
    "\n",
    "> Since $X^T X$ is psd, we can use the proof from 3.3 to say that  $X^T X + \\lambda I$ is positive definite for $\\lambda > 0$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2.Rewrite $X^{T}Xw+\\lambda Iw=X^{T}y$ as $w=\\frac{1}{\\lambda}(X^{T}y-X^{T}Xw)$.\n",
    "Based on this, show that we can write $w=X^{T}\\alpha$ for some $\\alpha$,\n",
    "and give an expression for $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> $X^{T}Xw+\\lambda Iw=X^{T}y$\n",
    "\n",
    "> $\\lambda \\mathbb I w = X^T y - X^T X w$\n",
    "\n",
    "> $w = \\frac {1}{\\lambda} (X^T y - X^T X w)$\n",
    "\n",
    "> $w = X^T \\lbrack \\frac {1}{\\lambda} (y-Xw) \\rbrack $\n",
    "\n",
    "> $w = X^T \\alpha $\n",
    "\n",
    "> Thus, $\\alpha = \\frac {1}{\\lambda} (y-Xw)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Based on the fact that $w=X^{T}\\alpha$, explain why we say w is ''in\n",
    "the span of the data.''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $X^T \\in \\mathbf R^{d \\times n}$, and $\\alpha \\in \\mathbf R^n$. Thus, $w \\in \\mathbf R^d$. \n",
    "\n",
    "> $w = X^T \\alpha$\n",
    "\n",
    "> $\\Rightarrow w = \n",
    "\\begin{pmatrix} \n",
    "|& | & & | \\\\\n",
    "x_1 & x_2 & \\cdots & x_n \\\\\n",
    "|& | & & |\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} \n",
    "\\alpha_1 \\\\\n",
    "\\alpha_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\alpha_n\n",
    "\\end{pmatrix}$\n",
    "\n",
    "> $\\Rightarrow w =\n",
    "\\begin{pmatrix} \n",
    "\\alpha_1 x_{11} + \\alpha_2 x_{21} + \\cdots + \\alpha_n x_{n1}  \\\\\n",
    "\\alpha_1 x_{12} + \\alpha_2 x_{22} + \\cdots + \\alpha_n x_{n2} \\\\\n",
    "\\vdots \\\\\n",
    "\\alpha_1 x_{1n} + \\alpha_2 x_{2n} + \\cdots + \\alpha_n x_{nn}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "> Since $w$ consists of a linear combination of the columns of $X$, which contains the data, we say that $w$ is \"in the span of the data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Show that $\\alpha=(\\lambda I+XX^{T})^{-1}y$. Note that $XX^{T}$\n",
    "is the kernel matrix for the standard vector dot product. (Hint: Replace\n",
    "$w$ by $X^{T}\\alpha$ in the expression for $\\alpha$, and then solve\n",
    "for $\\alpha$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> From 5.2, we have\n",
    "\n",
    "> $\\alpha = \\frac {1}{\\lambda} (y-Xw)$\n",
    "\n",
    "> $\\Rightarrow \\alpha = \\frac {1}{\\lambda} (y-X X^T \\alpha)$ (since $w=X^T \\alpha$, from 5.2)\n",
    "\n",
    "> $\\Rightarrow \\alpha = \\frac{y}{\\lambda} - \\frac{X X^T \\alpha}{\\lambda}$\n",
    "\n",
    "> $\\Rightarrow \\alpha + \\frac{X X^T \\alpha}{\\lambda} = \\frac {y} {\\lambda}$\n",
    "\n",
    "> $\\Rightarrow \\alpha \\lambda \\mathbb I + X X^T \\alpha = y$\n",
    "\n",
    "> $\\Rightarrow \\alpha (\\lambda \\mathbb I + X X^T) = y$\n",
    "\n",
    "> $\\Rightarrow \\alpha =  {(\\lambda \\mathbb I + X X^T)^{-1}} y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Give a kernelized expression for the $Xw$, the predicted values on\n",
    "the training points. (Hint: Replace $w$ by $X^{T}\\alpha$ and $\\alpha$\n",
    "by its expression in terms of the kernel matrix $XX^{T})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> $Xw$\n",
    "\n",
    "> $= X X^T \\alpha$      (from 5.2)\n",
    "\n",
    "> $= (X X^T) (\\lambda \\mathbb I + X X^T)^{-1} y$\n",
    "\n",
    "> $= K (\\lambda \\mathbb I + K)^{-1} y$ where $K$ is the Gram/kernel matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Give an expression for the prediction $f(x)=x^{T}w^{*}$ for a new\n",
    "point $x$, not in the training set. The expression should only involve\n",
    "$x$ via inner products with other $x$'s. [Hint: It is often convenient\n",
    "to define the column vector\n",
    "\n",
    "$$\n",
    "k_{x}=\\begin{pmatrix}x^{T}x_{1}\\\\\n",
    "\\vdots\\\\\n",
    "x^{T}x_{n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "to simplify the expression.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> $f(x) = x^T w^*$\n",
    "\n",
    "> $= x^T X^T \\alpha^*$\n",
    "\n",
    ">$=\\begin{pmatrix}\n",
    "-x_1- \\\\\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix} \n",
    "|& | & & | \\\\\n",
    "x_1 & x_2 & \\cdots & x_n \\\\\n",
    "|& | & & |\n",
    "\\end{pmatrix} \\alpha^*\n",
    "$\n",
    "\n",
    ">$=\\begin{pmatrix}\n",
    "x^T x_1 & x^T x_2 & \\cdots & x^T x_n \\\\\n",
    "\\end{pmatrix}\n",
    "(\\lambda \\mathbb I + X X^T)^{-1} y\n",
    "$\n",
    "\n",
    "> $= k_x^T (\\lambda \\mathbb I + X X^T)^{-1} y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. [Optional] Pegasos and SSGD for `2-regularized ERM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 7. Kernelized pegasos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Recall the SVM objective function\n",
    "\n",
    "$$\\min_{w\\in \\mathbb R^{n}}\\frac{\\lambda}{2}\\|w\\|^{2}+\\frac{1}{m}\\sum_{i=1}^{m}\\max\\left\\{ 0,1-y_{i}w^{T}x_{i}\\right\\}$$ \n",
    "\n",
    "and the Pegasos algorithm on the training set $\\left(x_{1},y_{1}\\right),\\ldots,(x_{n},y_{n})\\in \\mathbb R^{d}\\times\\left\\{ -1,1\\right\\} $.\n",
    "\n",
    "Note that in every step of Pegasos, we rescale $w^{(t)}$ by $\\left(1-\\eta^{(t)}\\lambda\\right)=\\left(1-\\frac{1}{t}\\right)\\in\\left(0,1\\right)$.\n",
    "This ``shrinks'' the entries of $w^{(t)}$ towards $0$, and it's\n",
    "due to the regularization term $\\frac{\\lambda}{2}\\|w\\|_{2}^{2}$ in\n",
    "the SVM objective function. Also note that if the example in a particular\n",
    "step, say $\\left(x_{j},y_{j}\\right)$, is not classified with the\n",
    "required margin (i.e. if we don't have margin $y_{j}w_{t}^{T}x_{j}\\ge1$),\n",
    "then we also add a multiple of $x_{j}$ to $w^{(t)}$ to end up with\n",
    "$w^{(t+1)}$. This part of the adjustment comes from the empirical\n",
    "risk. Since we initialize with $w^{(1)}=0$, we are guaranteed that\n",
    "we can always write\n",
    "\n",
    "$$w^{(t)}=\\sum_{i=1}^{n}\\alpha_{i}^{(t)}x_{i}$$\n",
    "\n",
    "after any number of steps $t$. When we kernelize Pegasos, we'll be\n",
    "tracking $\\alpha^{(t)}=(\\alpha_{1}^{(t)},\\ldots,\\alpha_{n}^{(t)})^{T}$\n",
    "directly, rather than $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Kernelize the expression for the margin. That is, show that $y_{j}\\left\\langle w^{(t)},x_{j}\\right\\rangle =y_{j}K_{j\\cdot}\\alpha^{(t)}$,\n",
    "where $k(x_{i},x_{j})=\\left\\langle x_{i},x_{j}\\right\\rangle $ and\n",
    "$K_{j\\cdot}$ denotes the $j$th row of the kernel matrix $K$ corresponding\n",
    "to kernel $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> $y_{j}\\left\\langle w^{(t)},x_{j}\\right\\rangle$\n",
    "\n",
    "> $= y_j \\langle \\sum_{i=1}^{n}\\alpha_i^{(t)} x_i, x_j \\rangle $\n",
    "\n",
    "> $= y_j \\sum_{i=1}^{n}\\langle \\alpha_i^{(t)} x_i, x_j \\rangle $\n",
    "\n",
    "> $= y_j \\sum_{i=1}^{n} \\alpha_i^{(t)} \\langle x_i, x_j \\rangle $\n",
    "\n",
    "> $= y_j \\sum_{i=1}^{n} \\alpha_i^{(t)} k (x_i, x_j) $\n",
    "\n",
    "> Since $K_j$ denotes the $j^{th}$ row of $K$, each entry in $K_j$ will be the inner product between $x_j$ and every other $x_i$ ($i=1...n$)\n",
    "\n",
    "> Thus, we can rewrite the above as:\n",
    "\n",
    ">$= y_j K_j \\alpha^{(t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Suppose that $w^{(t)}=\\sum_{i=1}^{n}\\alpha_{i}^{(t)}x_{i}$ and for\n",
    "the next step we have selected a point $\\left(x_{j},y_{j}\\right)$\n",
    "that does not have a margin violation. Give an update expression for\n",
    "$\\alpha^{(t+1)}$ so that $w^{(t+1)}=\\sum_{i=1}^{n}\\alpha_{i}^{(t+1)}x_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> When there is no margin violation, $w^{(t+1)}=(1-\\eta^{(t)}\\lambda)w^{(t)}$.\n",
    "\n",
    "> We also have $w^{(t)}=\\sum_{i=1}^{n}\\alpha_{i}^{(t)}x_{i}$.\n",
    "\n",
    "> Replacing the first value of $w^{(t)}$ in the second equation:\n",
    "\n",
    "> $\\Rightarrow w^{(t+1)} = \\sum_{i=1}^n (1-\\eta^{(t)}\\lambda) \\alpha_i ^{(t)} x_i$\n",
    "\n",
    "> Looking at the above equation, setting $\\alpha_i^{(t+1)} = (1-\\eta^{(t)} \\lambda) \\alpha_i^{(t)}$ yields $w^{(t+1)}=\\sum_{i=1}^{n}\\alpha_{i}^{(t+1)}x_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Repeat the previous problem, but for the case that $\\left(x_{j},y_{j}\\right)$\n",
    "has a margin violation. Then give the full pseudocode for kernelized\n",
    "Pegasos. You may assume that you receive the kernel matrix $K$ as\n",
    "input, along with the labels $y_{1},\\ldots,y_{n}\\in\\left\\{ -1,1\\right\\} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> When there is a margin violation, \n",
    "> $w^{(t+1)}=(1-\\eta^{(t)}\\lambda)w^{(t)}+\\eta_{t}y_{j}x_{j}$\n",
    "\n",
    "> We also have $w^{(t)}=\\sum_{i=1}^{n}\\alpha_{i}^{(t)}x_{i}$\n",
    "\n",
    "> Replacing the first value of $w^{(t)}$ in the second equation:\n",
    "\n",
    "> $\\Rightarrow w^{(t+1)} = \\sum_{i=1}^n (1-\\eta^{(t)}\\lambda) \\alpha_i ^{(t)} x_i + +\\eta_{t}y_{j}x_{j}$\n",
    "\n",
    "\n",
    "\n",
    "> $\\alpha_i^{(t+1)} = (1-\\eta^{(t)} \\lambda) \\alpha_i^{(t)}$ for $i \\neq j$\n",
    "\n",
    "> $\\alpha_i^{(t+1)} = ((1-\\eta^{(t)} \\lambda) \\alpha_i^{(t)} + \\eta_t y_j)$ for $i = j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Input: Training set$\\left(x_{1},y_{1}\\right),\\ldots,(x_{n},y_{n})\\in \\mathbf R^{d}\\times\\left\\{ -1,1\\right\\} $,  $\\lambda>0$ and kernel matrix $K$.\n",
    "\n",
    "> $\\alpha \\leftarrow (0,...,0) \\in \\mathbf R^n$ \n",
    "\n",
    "> $t \\leftarrow 0$\n",
    "\n",
    "> repeat\n",
    "\n",
    "> $~~~~t \\leftarrow t+1$\n",
    "\n",
    "> $~~~~\\eta^{(t)} \\leftarrow 1/(t \\lambda)$\n",
    "\n",
    "> $~~~~randomly~choose~j~in~1,...,n$\n",
    "\n",
    "> $~~~~for~all~i:$\n",
    "\n",
    "> $~~~~~~~~\\alpha_i \\leftarrow (1-\\eta \\lambda)\\alpha_i$\n",
    "\n",
    "> $~~~~if~y_j K_j \\alpha < 1:$\n",
    "\n",
    "> $~~~~~~~~\\alpha_j \\leftarrow \\alpha_j + \\eta y_j$\n",
    "\n",
    "> $~~~~else:$\n",
    "\n",
    "> $~~~~~~~~\\alpha_j \\leftarrow \\alpha_j$\n",
    "\n",
    "> until bored\n",
    "\n",
    "> return $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Kernel Methods: Let’s Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.In this section you will get the opportunity to code kernel ridge\n",
    "regression and, optionally, kernelized SVM. To speed things along,\n",
    "we've written a great deal of support code for you, which you can\n",
    "find in the Jupyter notebooks in the homework zip file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
