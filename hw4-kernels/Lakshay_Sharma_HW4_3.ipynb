{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Representer Theorem [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Let $M$ be a closed subspace of a Hilbert space $\\mathcal H$. For any $x\\in\\mathcal H$,\n",
    "let $m_{0}=Proj_{M}x$ be the projection of $x$ onto $M$. By the\n",
    "Projection Theorem, we know that $(x-m_{0})\\perp M$. Then by the\n",
    "Pythagorean Theorem, we know $\\|x\\|^{2}=\\|m_{0}\\|^{2}+\\|x-m_{0}\\|^{2}$.\n",
    "From this we concluded in lecture that $\\|m_{0}\\|\\le\\|x\\|$. Show\n",
    "that we have $\\|m_{0}\\|=\\|x\\|$ only when $m_{0}=x$. (Hint: Use the\n",
    "postive-definiteness of the inner product: $\\left\\langle x,x\\right\\rangle \\ge0$\n",
    "and $\\left\\langle x,x\\right\\rangle =0\\iff x=0$, and the fact that\n",
    "we're using the norm derived from such an inner product.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> By positive-definiteness of inner-product, we have the properties: $\\left\\langle x,x\\right\\rangle \\ge0$\n",
    "and $\\left\\langle x,x\\right\\rangle =0\\iff x=0$.\n",
    "\n",
    "> If $\\|m_0\\|=\\|x\\|$, \n",
    "\n",
    "> $\\Rightarrow \\langle x-m_0, x-m_0 \\rangle = 0$.\n",
    "\n",
    "> The above can only occur if $x=m_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Give the proof of the Representer Theorem in the case that $R$ is\n",
    "strictly increasing. That is, show that if $R$ is strictly increasing,\n",
    "then all minimizers have this form claimed. (Hint: Consider separately\n",
    "the cases that $\\|w\\|<\\|w^{*}\\|$ and the case $\\|w\\|=\\|w^{*}\\|$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Suppose that $R:\\mathbf R^{\\ge0}\\to\\mathbf R$ and $L:\\mathbf R^{n}\\to\\mathbf R$\n",
    "are both convex functions. Use properties of convex functions to show\n",
    "that $w\\mapsto L\\left(\\left\\langle w,\\psi(x_{1})\\right\\rangle ,\\ldots,\\left\\langle w,\\psi(x_{n})\\right\\rangle \\right)$\n",
    "is a convex function of $w$, and then that $J(w)$ is also convex\n",
    "function of $w$. For simplicity, you may assume that our feature\n",
    "space is $\\mathbf R^{d}$, rather than a generic Hilbert space. You may\n",
    "also use the fact that the composition of a convex function and an\n",
    "affine function is convex. That is:, suppose $f:\\mathbf R^{n}\\to\\mathbf R,\\ A\\in\\mathbf R^{n\\times m}$\n",
    "and $b\\in\\mathbf R^{n}.$ Define $g:\\mathbf R^{m}\\to\\mathbf R$ by $g(x)=f\\left(Ax+b\\right)$.\n",
    "Then if $f$ is convex, then so is $g$. From this exercise, we can\n",
    "conclude that if $L$ and $R$ are convex, then $J$ does have a minimizer\n",
    "of the form $w^{*}=\\sum_{i=1}^{n}\\alpha_{i}\\psi(x_{i})$, and if $R$\n",
    "is also strictly increasing, then all minimizers of $J$ have this\n",
    "form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> Consider $A \\in \\mathbf R^{n \\times m}$ is a matrix where each row $A_{i,:}=\\psi(x_i)$, and $w \\in \\mathbf R^m$.\n",
    "\n",
    "> Thus, $Aw \\in R^n$, where $Aw_i = \\langle w, \\psi(x_i) \\rangle$\n",
    "\n",
    "> We need to start by proving that $w\\mapsto L\\left(\\left\\langle w,\\psi(x_{1})\\right\\rangle ,\\ldots,\\left\\langle w,\\psi(x_{n})\\right\\rangle \\right)$ is a convex function of $w$.\n",
    "\n",
    "> Since $L$ maps from $w$ to the space of inner products $\\langle w,\\psi(x_{i})\\rangle$, we can rewrite the above equation as: $w\\mapsto L\\left(Aw \\right)$. This is a convex function, since we have been told that $L: \\mathbf R^n \\rightarrow \\mathbf R$ (for some arbitrary positive integer n) is a convex function. \n",
    "\n",
    "> Thus, we can say that $w \\mapsto L (Aw) = w\\mapsto L\\left(\\left\\langle w,\\psi(x_{1})\\right\\rangle ,\\ldots,\\left\\langle w,\\psi(x_{n})\\right\\rangle \\right)$ is a convex function of $w$.\n",
    "\n",
    "> Since $J(w)$ is of the form $R(\\|w\\|) + L\\left(\\left\\langle w,\\psi(x_{1})\\right\\rangle ,\\ldots,\\left\\langle w,\\psi(x_{n})\\right\\rangle \\right)$, if we can prove that $R(\\|w\\|)$ is convex function, we can say that $J(w)$ is convex, since the sum of two convex functions is a convex function.\n",
    "\n",
    "> Since $\\|.\\|$ is a convex function, $R(\\|w\\|)$ is convex.\n",
    "\n",
    "> Therefore, $J(w) = R(\\|w\\|) + L\\left(\\left\\langle w,\\psi(x_{1})\\right\\rangle ,\\ldots,\\left\\langle w,\\psi(x_{n})\\right\\rangle \\right)$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Ivanov and Tikhonov Regularization [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture there was a claim that the Ivanov and Tikhonov forms of\n",
    "ridge and lasso regression are equivalent. We will now prove a more\n",
    "general result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Tikhonov optimal implies Ivanov optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\phi:\\mathcal f\\to\\mathbf R$ be any performance measure of $f\\in\\mathcal f$,\n",
    "and let $\\Omega:\\mathcal f\\to\\mathbf R$ be any complexity measure. For example,\n",
    "for ridge regression over the linear hypothesis space $\\mathcal f=\\left\\{ f_{w}(x)=w^{T}x\\mid w\\in\\mathbf R^{d}\\right\\} $,\n",
    "we would have $\\phi(f_{w})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(w^{T}x_{i}-y_{i}\\right)^{2}$\n",
    "and $\\Omega(f_{w})=w^{T}w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Suppose that for some $\\lambda>0$ we have the Tikhonov regularization\n",
    "solution\n",
    "\n",
    "$$\n",
    "f^* =argmin_{f\\in\\mathcal F}\\left[\\phi(f)+\\lambda\\Omega(f)\\right].~~~~~~~~~~(1)\n",
    "$$\n",
    "\n",
    "Show that $f^*$ is also an Ivanov solution. That is, $\\exists r>0$   \n",
    "such that\n",
    "\n",
    "$$\n",
    "f^*= argmin_{\\substack{f\\in\\mathcal F}\n",
    "}\\phi(f)\\mbox{ subject to }\\Omega(f)\\le r.].~~~~~~~~~~(2)\n",
    "$$\n",
    "\n",
    "(Hint: Start by figuring out what $r$ should be. If you're stuck\n",
    "on this, ask for help. Then one approach is proof by contradiction:\n",
    "suppose $f^{*}$ is not the optimum in (2) and show\n",
    "that contradicts the fact that $f^{*}$ solves (1).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><b>Solution</b>\n",
    "\n",
    "> Let $r$ be the complexity measure of $f^*$, that is $r=\\Omega (f^*) = w^{*T}w^*$.\n",
    "\n",
    "> (expressing in the Ivanov solution form) Let us assume that there exists some other $\\tilde f_w$ such that $\\phi (\\tilde f) < \\phi (f^*)$, and $\\Omega(\\tilde f) \\leq r $\n",
    "\n",
    "> This would imply $\\phi (f^*) + r > \\phi (\\tilde f) + \\Omega(\\tilde f)$\n",
    "\n",
    "> $\\Rightarrow \\phi (f^*) + \\lambda w^{*T}w^* > \\phi (\\tilde f) + \\lambda \\tilde w^T \\tilde w$ (which is in the Tikhonov form) would contradict $(1)$.\n",
    "\n",
    "> This proves that Tikhonov optimal implies Ivanov optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 10.2 Ivanov optimal implies Tikhonov optimal (when we have Strong Duality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the converse, we will restrict our hypothesis space to a parametric\n",
    "set. That is, \n",
    "$\\mathcal F=\\left\\{ f_{w}(x):\\mathcal X \\to\\mathbf R\\mid w\\in\\mathbf R^{d}\\right\\}$ .\n",
    "\n",
    "So we will now write $\\phi$ and $\\Omega$ as functions of $w\\in\\mathbf R^{d}$. \n",
    "\n",
    "Let $w^{*}$ be a solution to the following Ivanov optimization problem:\n",
    "\\begin{eqnarray*}\n",
    "\\textrm{minimize} &  & \\phi(w)\\\\\n",
    "\\textrm{subject to} &  & \\Omega(w)\\le r.\n",
    "\\end{eqnarray*}\n",
    "Assume that strong duality holds for this optimization problem and\n",
    "that the dual solution is attained. Then we will show that there exists\n",
    "a $\\lambda\\ge0$ such that $ w^*=argmin_{w\\in\\mathbf R^{d}}\\left[\\phi(w)+\\lambda\\Omega(w)\\right].$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Write the Lagrangian $L(w,\\lambda)$ for the Ivanov optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> $L(w,\\lambda) = \\phi(w) + \\lambda(\\Omega(w)-r)$ such that $\\Omega(w)-r \\leq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Write the dual optimization problem in terms of the dual objective\n",
    "function $g(\\lambda)$, and give an expression for $g(\\lambda)$.\n",
    "[Writing $g(\\lambda)$ as an optimization problem is expected -\n",
    "don't try to solve it.] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> The dual optimization problem can be expressed in terms of the dual objective function $g(\\lambda)$ as follows:\n",
    "\n",
    "> $$ sup_{\\lambda \\geq 0}~~ g(\\lambda)$$\n",
    "\n",
    "> $$=  sup_{\\lambda \\geq 0}~~ inf_w~~ L(w,\\lambda)$$\n",
    "\n",
    "> $$= sup_{\\lambda \\geq 0}~~ inf_w~~ \\phi(w) + \\lambda(\\Omega(w)-r)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.We assumed that the dual solution is attained, so let $\\lambda^{*}=argmax_{\\lambda\\ge0}g(\\lambda)$. We also assumed strong duality, which implies $\\phi(w^{*})=g(\\lambda^{*})$.\n",
    "Show that the minimum in the expression for $g(\\lambda^{*})$ is attained\n",
    "at $w^{*}$. [Hint: You can use the same approach we used when we\n",
    "derived that strong duality implies complementary slackness]. Conclude the proof by showing that for the choice of\n",
    "$\\lambda=\\lambda^{*}$, we have $w^*=argmin_{w\\in\\mathbf R^{d}}\\left[\\phi(w)+\\lambda\\Omega(w)\\right].$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Solution: </b>\n",
    "\n",
    "> Since we assumedstrong duality,\n",
    "\n",
    "> $\\phi(w^*) = g(\\lambda^*)$\n",
    "\n",
    "> $~~~~~~~~~ = inf_w L(w, \\lambda^*)$\n",
    "\n",
    "> $~~~~~~~~~ = inf_w \\phi(w) + \\lambda^*[\\Omega(w)-r]$\n",
    "\n",
    "> $~~~~~~~~~ \\leq L(w^*, \\lambda^*)$\n",
    "\n",
    "\n",
    "> $~~~~~~~~~ = \\phi(w^*) + \\underbrace{\\lambda^* [\\Omega(w^*)-r]}_{\\leq 0}$\n",
    "\n",
    "> $~~~~~~~~~ \\leq \\phi(w^*)$\n",
    "\n",
    "> The only way for the above to be possible is if all conditions/equations involving inequalities are equalities.\n",
    "\n",
    "> Thus, \n",
    "\n",
    "> $w^* = argmin_{w} \\phi(w) + \\lambda^*[\\Omega(w)-r]$\n",
    "\n",
    "> $~~~~= argmin_{w} \\phi(w) + \\lambda^*[\\Omega(w)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4,The conclusion of the previous problem allows $\\lambda=0$, which\n",
    "means we're not actually regularizing at all. To ensure we get a proper\n",
    "Ivanov regularization problem, we need an additional assumption. The\n",
    "one below is taken from \\cite{kloft2009efficient}:\n",
    "$$\n",
    "\\inf_{w\\in\\mathbf R^{d}}\\phi(w)<\\inf_{\\substack{w\\in \\mathbf R^{d}\\\\\n",
    "\\Omega(w)\\le r\n",
    "}\n",
    "}\\phi(w)\n",
    "$$\n",
    "\n",
    "Note that this is a rather intuitive condition: it is simply saying\n",
    "that we can fit the training data better [strictly better] if\n",
    "we don't use any regularization. With this additional condition, show\n",
    "that $w^*=argmin_{w\\in\\mathbf R^{d}}\\left[\\phi(w)+\\lambda\\Omega(w)\\right]$\n",
    "for some $\\lambda>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution:</b>\n",
    "\n",
    "> The dual solution is defined as $g(\\lambda) = \\inf\\limits_w~~ \\phi(w) + \\lambda[\\Omega(w)-r]$.\n",
    "\n",
    "> If $\\lambda=0$, $g(0) = \\inf\\limits_w~~ \\phi(w)$\n",
    "\n",
    "> As per the assumption specified in the problem, \n",
    "\n",
    "> $\\Rightarrow \\inf\\limits_w~~ \\phi(w) < \\inf \\limits_{w\\in \\mathbf R^{d}~~and~~\\Omega(w) \\le r} \\phi(w) = g(\\lambda^*)$\n",
    "\n",
    "> Thus, we can conclude that $\\lambda^* \\neq 0$. Since the Lagrangian duality conditions specify that $\\lambda \\geq 0$, it has to be the case that $\\lambda^* > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Ivanov implies Tikhonov for Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that Ivanov implies Tikhonov for the ridge regression problem\n",
    "(square loss with $\\ell_{2}$ regularization), we need to demonstrate\n",
    "strong duality and that the dual optimum is attained. Both of these\n",
    "things are implied by Slater's constraint qualifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Show that the Ivanov form of ridge regression is a convex optimization\n",
    "problem with a strictly feasible point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Solution</b>\n",
    "\n",
    "> The Ivanov form of ridge regression can be expressed as:\n",
    "\n",
    "> $minimize~~~(y_i - w^T x_i)^2$\n",
    "\n",
    "> $subject~to~~\\|w\\|_2^2 \\leq r$\n",
    "\n",
    "\n",
    "> It can easily be seen that the objective $(y_i - w^T x_i)^2$ is convex.\n",
    "\n",
    "> The constraint function $\\|w\\|_2^2 \\leq r$ is also convex. Since this is an affine function, $w=0$ satisfies the condition sufficient to satisfy Slater's condition. \n",
    "\n",
    "> Thus, Ivanov form of ridge regression is a convex optimization problem with a strictly feasible point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 11. [Optional] Novelty Detection"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
